{"cells":[{"cell_type":"markdown","metadata":{"id":"_xnMOsbqHz61"},"source":["# Day/Night style transfer with a conditional GAN (Pix2pix architecture)"]},{"cell_type":"markdown","metadata":{},"source":["**This notebook has been based on the Tensorflow Pix2pix tutorial which has an Apache licensence 2.0**"]},{"cell_type":"markdown","metadata":{"id":"e1_Y75QXJS6h"},"source":["## Import modules and define global variables"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:19:07.367432Z","iopub.status.busy":"2022-02-03T12:19:07.366778Z","iopub.status.idle":"2022-02-03T12:19:10.076089Z","shell.execute_reply":"2022-02-03T12:19:10.075313Z","shell.execute_reply.started":"2022-02-03T12:19:07.367130Z"},"id":"YfIk2es3hJEd","trusted":true},"outputs":[],"source":["import tensorflow as tf\n","\n","import os\n","import pathlib\n","import time\n","import datetime\n","import glob\n","\n","from matplotlib import pyplot as plt\n","from IPython import display"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:19:10.078249Z","iopub.status.busy":"2022-02-03T12:19:10.077967Z","iopub.status.idle":"2022-02-03T12:19:10.085121Z","shell.execute_reply":"2022-02-03T12:19:10.084274Z","shell.execute_reply.started":"2022-02-03T12:19:10.078207Z"},"id":"2CbTEt448b4R","trusted":true},"outputs":[],"source":["# The batch size is recommended to be between 1 and 10 based on the paper\n","BATCH_SIZE = 1\n","# Each image is 256x256 in size\n","IMG_WIDTH = 256\n","IMG_HEIGHT = 256"]},{"cell_type":"markdown","metadata":{},"source":["## Load training and testing images and create image couples from single images"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:19:10.087103Z","iopub.status.busy":"2022-02-03T12:19:10.086536Z","iopub.status.idle":"2022-02-03T12:23:36.593762Z","shell.execute_reply":"2022-02-03T12:23:36.592302Z","shell.execute_reply.started":"2022-02-03T12:19:10.087047Z"},"id":"vJ2sO8Izg7QV","trusted":true},"outputs":[],"source":["image_list = []\n","test_list = []\n","\n","for filename in glob.glob('../input/carla-parired-images/data_carla/day_train/*'):\n","    img = tf.io.read_file(filename)\n","    img = tf.image.decode_png(img, channels=3)\n","    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n","    img = tf.cast(img, tf.float32)\n","    image_list.append(img)\n","    \n","for count, filename in enumerate(glob.glob('../input/carla-parired-images/data_carla/night_train/*')):\n","    img = tf.io.read_file(filename)\n","    img = tf.image.decode_png(img, channels=3)\n","    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n","    img = tf.cast(img, tf.float32)\n","    image_list[count] = image_list[count], img\n","    \n","    \n","for filename in glob.glob('../input/carla-parired-images/data_carla/day_val/*'):\n","    img = tf.io.read_file(filename)\n","    img = tf.image.decode_png(img, channels=3)\n","    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n","    img = tf.cast(img, tf.float32)\n","    test_list.append(img)\n","    \n","for count, filename in enumerate(glob.glob('../input/carla-parired-images/data_carla/night_val/*')):\n","    img = tf.io.read_file(filename)\n","    img = tf.image.decode_png(img, channels=3)\n","    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n","    img = tf.cast(img, tf.float32)\n","    test_list[count] = test_list[count], img\n","print(len(image_list))\n","print(len(image_list[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:36.596611Z","iopub.status.busy":"2022-02-03T12:23:36.596039Z","iopub.status.idle":"2022-02-03T12:23:36.601182Z","shell.execute_reply":"2022-02-03T12:23:36.600294Z","shell.execute_reply.started":"2022-02-03T12:23:36.596568Z"},"id":"aO9ZAGH5K3SY","trusted":true},"outputs":[],"source":["# load images seperatley from the joined list\n","def load(image_file):\n","    return image_file[0], image_file[1] # switch 0 and 1 to perform night to day transfer"]},{"cell_type":"markdown","metadata":{"id":"r5ByHTlfE06P"},"source":["Plot a sample of the input (day image) and real (night image) images:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:36.603043Z","iopub.status.busy":"2022-02-03T12:23:36.602771Z","iopub.status.idle":"2022-02-03T12:23:37.048848Z","shell.execute_reply":"2022-02-03T12:23:37.048162Z","shell.execute_reply.started":"2022-02-03T12:23:36.603006Z"},"id":"4OLHMpsQ5aOv","trusted":true},"outputs":[],"source":["inp, re = load(image_list[1])\n","# putting pixels between 0 and 1 for matplotlib\n","plt.figure()\n","plt.imshow(inp / 255.0)\n","plt.figure()\n","plt.imshow(re / 255.0)"]},{"cell_type":"markdown","metadata":{"id":"PVuZQTfI_c-s"},"source":["As described in the [pix2pix paper](https://arxiv.org/abs/1611.07004), you need to apply random jittering and mirroring to preprocess the training set.\n","\n","Define several functions that:\n","\n","1. Resize each `256 x 256` image to a larger height and widthâ€”`286 x 286`.\n","2. Randomly crop it back to `256 x 256`.\n","3. Randomly flip the image horizontally i.e. left to right (random mirroring).\n","4. Normalize the images to the `[-1, 1]` range."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:37.050613Z","iopub.status.busy":"2022-02-03T12:23:37.050153Z","iopub.status.idle":"2022-02-03T12:23:37.056581Z","shell.execute_reply":"2022-02-03T12:23:37.055826Z","shell.execute_reply.started":"2022-02-03T12:23:37.050572Z"},"id":"rwwYQpu9FzDu","trusted":true},"outputs":[],"source":["def resize(input_image, real_image, height, width):\n","    input_image = tf.image.resize(input_image, [height, width],\n","                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","    real_image = tf.image.resize(real_image, [height, width],\n","                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","\n","    return input_image, real_image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:37.058483Z","iopub.status.busy":"2022-02-03T12:23:37.057991Z","iopub.status.idle":"2022-02-03T12:23:37.066034Z","shell.execute_reply":"2022-02-03T12:23:37.065145Z","shell.execute_reply.started":"2022-02-03T12:23:37.058440Z"},"id":"Yn3IwqhiIszt","trusted":true},"outputs":[],"source":["def random_crop(input_image, real_image):\n","    stacked_image = tf.stack([input_image, real_image], axis=0)\n","    cropped_image = tf.image.random_crop(\n","    stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n","\n","    return cropped_image[0], cropped_image[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:37.067808Z","iopub.status.busy":"2022-02-03T12:23:37.067535Z","iopub.status.idle":"2022-02-03T12:23:37.077938Z","shell.execute_reply":"2022-02-03T12:23:37.077132Z","shell.execute_reply.started":"2022-02-03T12:23:37.067772Z"},"id":"muhR2cgbLKWW","trusted":true},"outputs":[],"source":["# Normalizing the images to [-1, 1]\n","def normalize(input_image, real_image):\n","    input_image = (input_image / 127.5) - 1\n","    real_image = (real_image / 127.5) - 1\n","\n","    return input_image, real_image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:37.079730Z","iopub.status.busy":"2022-02-03T12:23:37.079448Z","iopub.status.idle":"2022-02-03T12:23:37.087978Z","shell.execute_reply":"2022-02-03T12:23:37.087218Z","shell.execute_reply.started":"2022-02-03T12:23:37.079677Z"},"id":"fVQOjcPVLrUc","trusted":true},"outputs":[],"source":["def random_jitter(input_image, real_image):\n","    # Resizing to 286x286\n","    input_image, real_image = resize(input_image, real_image, 286, 286)\n","\n","    # Random cropping back to 256x256\n","    input_image, real_image = random_crop(input_image, real_image)\n","    \n","    if tf.random.uniform(()) > 0.5:\n","        # Random mirroring\n","        input_image = tf.image.flip_left_right(input_image)\n","        real_image = tf.image.flip_left_right(real_image)\n","\n","    return input_image, real_image"]},{"cell_type":"markdown","metadata":{"id":"wfAQbzy799UV"},"source":["You can inspect some of the preprocessed output:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:37.091677Z","iopub.status.busy":"2022-02-03T12:23:37.091408Z","iopub.status.idle":"2022-02-03T12:23:37.409738Z","shell.execute_reply":"2022-02-03T12:23:37.409047Z","shell.execute_reply.started":"2022-02-03T12:23:37.091641Z"},"id":"n0OGdi6D92kM","trusted":true},"outputs":[],"source":["plt.figure(figsize=(6, 6))\n","for i in range(4):\n","    rj_inp, rj_re = random_jitter(inp, re)\n","    plt.subplot(2, 2, i + 1)\n","    plt.imshow(rj_inp / 255.0)\n","    plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"3E9LGq3WBmsh"},"source":["Having checked that the loading and preprocessing works, let's define a couple of helper functions that load and preprocess the training and test sets:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:37.411185Z","iopub.status.busy":"2022-02-03T12:23:37.410814Z","iopub.status.idle":"2022-02-03T12:23:37.417524Z","shell.execute_reply":"2022-02-03T12:23:37.416647Z","shell.execute_reply.started":"2022-02-03T12:23:37.411140Z"},"id":"tyaP4hLJ8b4W","trusted":true},"outputs":[],"source":["def load_image_train(image_file):\n","    input_image, real_image = load(image_file)\n","    input_image, real_image = random_jitter(input_image, real_image)\n","    input_image, real_image = normalize(input_image, real_image)\n","\n","    return input_image, real_image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:37.419624Z","iopub.status.busy":"2022-02-03T12:23:37.419350Z","iopub.status.idle":"2022-02-03T12:23:37.425687Z","shell.execute_reply":"2022-02-03T12:23:37.424680Z","shell.execute_reply.started":"2022-02-03T12:23:37.419587Z"},"id":"VB3Z6D_zKSru","trusted":true},"outputs":[],"source":["def load_image_test(image_file):\n","    input_image, real_image = load(image_file)\n","    input_image, real_image = normalize(input_image, real_image)\n","    \n","    return input_image, real_image"]},{"cell_type":"markdown","metadata":{"id":"PIGN6ouoQxt3"},"source":["## Build an input pipeline with `tf.data`"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:37.427727Z","iopub.status.busy":"2022-02-03T12:23:37.427458Z","iopub.status.idle":"2022-02-03T12:23:43.231458Z","shell.execute_reply":"2022-02-03T12:23:43.230653Z","shell.execute_reply.started":"2022-02-03T12:23:37.427693Z"},"id":"SQHmYSmk8b4b","trusted":true},"outputs":[],"source":["train_dataset = tf.data.Dataset.from_tensor_slices(image_list)\n","# the tf.data.AUTOTUNE lets tensorflow choose automatically the number of elements to fetch from\n","# the training by another thread set prior to them being needed (in order to speed up the training)\n","train_dataset = train_dataset.map(load_image_train,\n","                                  num_parallel_calls=tf.data.AUTOTUNE)\n","train_dataset = train_dataset.batch(BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:43.233551Z","iopub.status.busy":"2022-02-03T12:23:43.232641Z","iopub.status.idle":"2022-02-03T12:23:44.673616Z","shell.execute_reply":"2022-02-03T12:23:44.672843Z","shell.execute_reply.started":"2022-02-03T12:23:43.233518Z"},"id":"MS9J0yA58b4g","trusted":true},"outputs":[],"source":["test_dataset = tf.data.Dataset.from_tensor_slices(test_list)\n","test_dataset = test_dataset.map(load_image_test)\n","test_dataset = test_dataset.batch(BATCH_SIZE)"]},{"cell_type":"markdown","metadata":{"id":"THY-sZMiQ4UV"},"source":["## Build the generator\n","\n","The generator of your pix2pix cGAN is a _modified_ U-Net. A U-Net consists of an encoder (downsampler) and decoder (upsampler).\n","\n","- Each block in the encoder is: Convolution -> Batch normalization -> Leaky ReLU\n","- Each block in the decoder is: Transposed convolution -> Batch normalization -> Dropout (applied to the first 3 blocks) -> ReLU\n","- There are skip connections between the encoder and decoder (as in the U-Net)."]},{"cell_type":"markdown","metadata":{"id":"4MQPuBCgtldI"},"source":["Define the downsampler (encoder):"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:44.675673Z","iopub.status.busy":"2022-02-03T12:23:44.675153Z","iopub.status.idle":"2022-02-03T12:23:44.679960Z","shell.execute_reply":"2022-02-03T12:23:44.679289Z","shell.execute_reply.started":"2022-02-03T12:23:44.675633Z"},"id":"tqqvWxlw8b4l","trusted":true},"outputs":[],"source":["OUTPUT_CHANNELS = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:44.682187Z","iopub.status.busy":"2022-02-03T12:23:44.681637Z","iopub.status.idle":"2022-02-03T12:23:44.690120Z","shell.execute_reply":"2022-02-03T12:23:44.689431Z","shell.execute_reply.started":"2022-02-03T12:23:44.682149Z"},"id":"3R09ATE_SH9P","trusted":true},"outputs":[],"source":["def downsample(filters, size, apply_batchnorm=True):\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","\n","    result = tf.keras.Sequential()\n","    result.add(\n","        tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n","                             kernel_initializer=initializer, use_bias=False))\n","\n","    if apply_batchnorm:\n","        result.add(tf.keras.layers.BatchNormalization())\n","\n","    result.add(tf.keras.layers.LeakyReLU())\n","\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:44.692527Z","iopub.status.busy":"2022-02-03T12:23:44.691642Z","iopub.status.idle":"2022-02-03T12:23:46.975741Z","shell.execute_reply":"2022-02-03T12:23:46.974931Z","shell.execute_reply.started":"2022-02-03T12:23:44.692488Z"},"id":"a6_uCZCppTh7","trusted":true},"outputs":[],"source":["down_model = downsample(3, 4)\n","down_result = down_model(tf.expand_dims(inp, 0))\n","print(down_result.shape)"]},{"cell_type":"markdown","metadata":{"id":"aFI_Pa52tjLl"},"source":["Define the upsampler (decoder):"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:46.977964Z","iopub.status.busy":"2022-02-03T12:23:46.977132Z","iopub.status.idle":"2022-02-03T12:23:46.985868Z","shell.execute_reply":"2022-02-03T12:23:46.985133Z","shell.execute_reply.started":"2022-02-03T12:23:46.977918Z"},"id":"nhgDsHClSQzP","trusted":true},"outputs":[],"source":["def upsample(filters, size, apply_dropout=False):\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","\n","    result = tf.keras.Sequential()\n","    result.add(\n","        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n","                                    padding='same',\n","                                    kernel_initializer=initializer,\n","                                    use_bias=False))\n","\n","    result.add(tf.keras.layers.BatchNormalization())\n","\n","    if apply_dropout:\n","        result.add(tf.keras.layers.Dropout(0.5))\n","\n","    result.add(tf.keras.layers.ReLU())\n","\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:46.987573Z","iopub.status.busy":"2022-02-03T12:23:46.987178Z","iopub.status.idle":"2022-02-03T12:23:47.104344Z","shell.execute_reply":"2022-02-03T12:23:47.103552Z","shell.execute_reply.started":"2022-02-03T12:23:46.987536Z"},"id":"mz-ahSdsq0Oc","trusted":true},"outputs":[],"source":["up_model = upsample(3, 4)\n","up_result = up_model(down_result)\n","print (up_result.shape)"]},{"cell_type":"markdown","metadata":{"id":"ueEJyRVrtZ-p"},"source":["Define the generator with the downsampler and the upsampler:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:47.106199Z","iopub.status.busy":"2022-02-03T12:23:47.105887Z","iopub.status.idle":"2022-02-03T12:23:47.118040Z","shell.execute_reply":"2022-02-03T12:23:47.117245Z","shell.execute_reply.started":"2022-02-03T12:23:47.106158Z"},"id":"lFPI4Nu-8b4q","trusted":true},"outputs":[],"source":["def Generator():\n","    inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n","\n","    down_stack = [\n","        downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n","        downsample(128, 4),  # (batch_size, 64, 64, 128)\n","        downsample(256, 4),  # (batch_size, 32, 32, 256)\n","        downsample(512, 4),  # (batch_size, 16, 16, 512)\n","        downsample(512, 4),  # (batch_size, 8, 8, 512)\n","        downsample(512, 4),  # (batch_size, 4, 4, 512)\n","        downsample(512, 4),  # (batch_size, 2, 2, 512)\n","        downsample(512, 4),  # (batch_size, 1, 1, 512)\n","      ]\n","\n","    up_stack = [\n","        upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n","        upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n","        upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n","        upsample(512, 4),  # (batch_size, 16, 16, 1024)\n","        upsample(256, 4),  # (batch_size, 32, 32, 512)\n","        upsample(128, 4),  # (batch_size, 64, 64, 256)\n","        upsample(64, 4),  # (batch_size, 128, 128, 128)\n","    ]\n","\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n","                                         strides=2,\n","                                         padding='same',\n","                                         kernel_initializer=initializer,\n","                                         activation='tanh')  # (batch_size, 256, 256, 3)\n","\n","    x = inputs\n","\n","    # Downsampling through the model\n","    skips = []\n","    for down in down_stack:\n","        x = down(x)\n","        skips.append(x)\n","\n","    skips = reversed(skips[:-1])\n","\n","    # Upsampling and establishing the skip connections\n","    for up, skip in zip(up_stack, skips):\n","        x = up(x)\n","        x = tf.keras.layers.Concatenate()([x, skip])\n","\n","    x = last(x)\n","\n","    return tf.keras.Model(inputs=inputs, outputs=x)"]},{"cell_type":"markdown","metadata":{"id":"dpDPEQXIAiQO"},"source":["### Define the generator loss\n","\n","Conditional GANs learn a structured loss that penalizes a possible structure that differs from the network output and the target image. In this case it penalises any couple of images that is not a real (non generated) day/night image couple.\n","\n","- The generator loss is a sigmoid cross-entropy loss of the generated images and an **array of ones**.\n","- The pix2pix paper also mentions the L1 loss (MAE) as it is said to give better results\n","- This allows the generated image to become structurally similar to the target image.\n","- The formula to calculate the total generator loss is `gan_loss + LAMBDA * l1_loss`, where `LAMBDA = 100`. This value was decided by the authors of the paper."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:47.120105Z","iopub.status.busy":"2022-02-03T12:23:47.119498Z","iopub.status.idle":"2022-02-03T12:23:47.129547Z","shell.execute_reply":"2022-02-03T12:23:47.128792Z","shell.execute_reply.started":"2022-02-03T12:23:47.119964Z"},"id":"cyhxTuvJyIHV","trusted":true},"outputs":[],"source":["LAMBDA = 100"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:47.131315Z","iopub.status.busy":"2022-02-03T12:23:47.130940Z","iopub.status.idle":"2022-02-03T12:23:47.138307Z","shell.execute_reply":"2022-02-03T12:23:47.137516Z","shell.execute_reply.started":"2022-02-03T12:23:47.131274Z"},"id":"Q1Xbz5OaLj5C","trusted":true},"outputs":[],"source":["loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:47.140795Z","iopub.status.busy":"2022-02-03T12:23:47.139998Z","iopub.status.idle":"2022-02-03T12:23:47.146554Z","shell.execute_reply":"2022-02-03T12:23:47.145684Z","shell.execute_reply.started":"2022-02-03T12:23:47.140768Z"},"id":"90BIcCKcDMxz","trusted":true},"outputs":[],"source":["def generator_loss(disc_generated_output, gen_output, target):\n","    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n","\n","    # Mean absolute error\n","    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n","\n","    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n","\n","    return total_gen_loss, gan_loss, l1_loss"]},{"cell_type":"markdown","metadata":{"id":"ZTKZfoaoEF22"},"source":["## Build the discriminator\n","\n","The discriminator in the pix2pix cGAN is a convolutional PatchGAN classifierâ€”it tries to classify if each image _patch_ is real or not real (which means that the neural network has less parameters which implies faster training).\n","\n","- Each block in the discriminator is: Convolution -> Batch normalization -> Leaky ReLU.\n","- The shape of the output after the last layer is `(batch_size, 30, 30, 1)`.\n","- Each `30 x 30` image patch of the output classifies a `70 x 70` portion of the input image.\n","- The discriminator receives 2 inputs: \n","    - The input image and the target image, which it should classify as real.\n","    - The input image and the generated image (the output of the generator), which it should classify as fake.\n","    - Use `tf.concat([inp, tar], axis=-1)` to concatenate these 2 inputs together."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:47.148588Z","iopub.status.busy":"2022-02-03T12:23:47.148079Z","iopub.status.idle":"2022-02-03T12:23:47.159725Z","shell.execute_reply":"2022-02-03T12:23:47.158981Z","shell.execute_reply.started":"2022-02-03T12:23:47.148524Z"},"id":"ll6aNeQx8b4v","trusted":true},"outputs":[],"source":["def Discriminator():\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","\n","    inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n","    tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n","\n","    x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 256, 256, channels*2)\n","\n","    down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n","    down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n","    down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n","\n","    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n","    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n","                                kernel_initializer=initializer,\n","                                use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n","\n","    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n","\n","    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n","\n","    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n","\n","    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n","                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n","\n","    return tf.keras.Model(inputs=[inp, tar], outputs=last)"]},{"cell_type":"markdown","metadata":{"id":"AOqg1dhUAWoD"},"source":["### Define the discriminator loss\n","\n","- The `discriminator_loss` function takes 2 inputs: **real images** and **generated images**.\n","- `real_loss` is a sigmoid cross-entropy loss of the **real images** and an **array of ones(since these are the real images)**.\n","- `generated_loss` is a sigmoid cross-entropy loss of the **generated images** and an **array of zeros (since these are the fake images)**.\n","- The `total_loss` is the sum of `real_loss` and `generated_loss`."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:47.162297Z","iopub.status.busy":"2022-02-03T12:23:47.162047Z","iopub.status.idle":"2022-02-03T12:23:47.169311Z","shell.execute_reply":"2022-02-03T12:23:47.168571Z","shell.execute_reply.started":"2022-02-03T12:23:47.162271Z"},"id":"wkMNfBWlT-PV","trusted":true},"outputs":[],"source":["def discriminator_loss(disc_real_output, disc_generated_output):\n","    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n","\n","    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n","\n","    total_disc_loss = real_loss + generated_loss\n","\n","    return total_disc_loss"]},{"cell_type":"markdown","metadata":{"id":"0FMYgY_mPfTi"},"source":["## Define the optimizers and a checkpoint-saver\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:33:09.659870Z","iopub.status.busy":"2022-02-03T12:33:09.659202Z","iopub.status.idle":"2022-02-03T12:33:10.218726Z","shell.execute_reply":"2022-02-03T12:33:10.217937Z","shell.execute_reply.started":"2022-02-03T12:33:09.659824Z"},"trusted":true},"outputs":[],"source":["generator = Generator()\n","discriminator = Discriminator()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:23:47.738359Z","iopub.status.busy":"2022-02-03T12:23:47.738095Z","iopub.status.idle":"2022-02-03T12:23:47.744702Z","shell.execute_reply":"2022-02-03T12:23:47.743071Z","shell.execute_reply.started":"2022-02-03T12:23:47.738324Z"},"id":"lbHFNexF0x6O","trusted":true},"outputs":[],"source":["# the learning rate and the beta_1 were chosen based on the paper\n","generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:33:20.119672Z","iopub.status.busy":"2022-02-03T12:33:20.119388Z","iopub.status.idle":"2022-02-03T12:33:20.125426Z","shell.execute_reply":"2022-02-03T12:33:20.124651Z","shell.execute_reply.started":"2022-02-03T12:33:20.119640Z"},"id":"WJnftd5sQsv6","trusted":true},"outputs":[],"source":["checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n","                                 discriminator_optimizer=discriminator_optimizer,\n","                                 generator=generator,\n","                                 discriminator=discriminator)"]},{"cell_type":"markdown","metadata":{"id":"Rw1fkAczTQYh"},"source":["## Generate images\n","\n","Write a function to plot some images during training.\n","\n","- Pass images from the test set to the generator.\n","- The generator will then translate the input image into the output.\n","- The last step is to plot the predictions and _voila_!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:33:20.128011Z","iopub.status.busy":"2022-02-03T12:33:20.127457Z","iopub.status.idle":"2022-02-03T12:33:20.136039Z","shell.execute_reply":"2022-02-03T12:33:20.135227Z","shell.execute_reply.started":"2022-02-03T12:33:20.127959Z"},"id":"RmdVsmvhPxyy","trusted":true},"outputs":[],"source":["def generate_images(model, test_input, tar):\n","    prediction = model(test_input, training=True)\n","    plt.figure(figsize=(15, 15))\n","\n","    display_list = [test_input[0], tar[0], prediction[0]]\n","    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n","\n","    for i in range(3):\n","        plt.subplot(1, 3, i+1)\n","        plt.title(title[i])\n","        # Getting the pixel values in the [0, 1] range to plot (they were in [-1,1] before).\n","        plt.imshow(display_list[i] * 0.5 + 0.5)\n","        plt.axis('off')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"gipsSEoZIG1a"},"source":["Test the function:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:33:20.137738Z","iopub.status.busy":"2022-02-03T12:33:20.137436Z","iopub.status.idle":"2022-02-03T12:33:21.032065Z","shell.execute_reply":"2022-02-03T12:33:21.031276Z","shell.execute_reply.started":"2022-02-03T12:33:20.137698Z"},"id":"8Fc4NzT-DgEx","trusted":true},"outputs":[],"source":["for example_input, example_target in test_dataset.take(1):\n","    generate_images(generator, example_input, example_target)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:33:21.034766Z","iopub.status.busy":"2022-02-03T12:33:21.034262Z","iopub.status.idle":"2022-02-03T12:33:21.040011Z","shell.execute_reply":"2022-02-03T12:33:21.039256Z","shell.execute_reply.started":"2022-02-03T12:33:21.034727Z"},"id":"xNNMDBNH12q-","trusted":true},"outputs":[],"source":["# setup a log directory for the losses to TensorBoard.\n","log_dir=\"logs/\"\n","\n","summary_writer = tf.summary.create_file_writer(\n","  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"]},{"cell_type":"markdown","metadata":{},"source":["define a simple training step"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:33:21.041951Z","iopub.status.busy":"2022-02-03T12:33:21.041419Z","iopub.status.idle":"2022-02-03T12:33:21.054017Z","shell.execute_reply":"2022-02-03T12:33:21.053222Z","shell.execute_reply.started":"2022-02-03T12:33:21.041895Z"},"id":"KBKUV2sKXDbY","trusted":true},"outputs":[],"source":["def train_step(input_image, target, step):\n","    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","        # generate an output image from the generator\n","        gen_output = generator(input_image, training=True)\n","    \n","        # dicriminator gets input and ground truth images, and then input and generated images\n","        disc_real_output = discriminator([input_image, target], training=True)\n","        disc_generated_output = discriminator([input_image, gen_output], training=True)\n","    \n","        # compute losses\n","        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n","        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n","\n","    generator_gradients = gen_tape.gradient(gen_total_loss,\n","                                          generator.trainable_variables)\n","    discriminator_gradients = disc_tape.gradient(disc_loss,\n","                                               discriminator.trainable_variables)\n","    # do a gradient step using the optimizers\n","    generator_optimizer.apply_gradients(zip(generator_gradients,\n","                                          generator.trainable_variables))\n","    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n","                                              discriminator.trainable_variables))\n","    # save losses values for tensorboard\n","    with summary_writer.as_default():\n","        tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//500)\n","        tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//500)\n","        tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//500)\n","        tf.summary.scalar('disc_loss', disc_loss, step=step//500)"]},{"cell_type":"markdown","metadata":{},"source":["The training loop (dependent on number of steps not epochs!)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:33:21.056657Z","iopub.status.busy":"2022-02-03T12:33:21.056011Z","iopub.status.idle":"2022-02-03T12:33:21.065439Z","shell.execute_reply":"2022-02-03T12:33:21.064661Z","shell.execute_reply.started":"2022-02-03T12:33:21.056618Z"},"id":"GFyPlBWv1B5j","trusted":true},"outputs":[],"source":["def fit(train_ds, test_ds, steps):\n","    # get examples from the test set to show results after each 1000 steps\n","    example_input, example_target = next(iter(test_ds.take(1)))\n","    start = time.time()\n","    \n","    for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():\n","        # clear output every 1000 steps to show newer results\n","        if (step) % 1000 == 0:\n","            display.clear_output(wait=True)\n","        \n","            if step != 0:\n","                print(f'Time taken for 1000 steps: {time.time()-start:.2f} sec\\n')\n","\n","            start = time.time()\n","        \n","            generate_images(generator, example_input, example_target)\n","            print(f\"Step: {step//1000}k\")\n","        \n","        # do a training step\n","        train_step(input_image, target, step)\n","\n","        # print a \".\" every 10 steps to show progression\n","        if (step+1) % 10 == 0:\n","            print('.', end='', flush=True)\n","\n","\n","        # Save (checkpoint) the model every 5k steps\n","        if (step + 1) % 5000 == 0:\n","            checkpoint.save(file_prefix=checkpoint_prefix)"]},{"cell_type":"markdown","metadata":{"id":"wozqyTh2wmCu"},"source":["Code to vizualise a tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:33:21.068429Z","iopub.status.busy":"2022-02-03T12:33:21.067631Z","iopub.status.idle":"2022-02-03T12:33:24.718018Z","shell.execute_reply":"2022-02-03T12:33:24.717143Z","shell.execute_reply.started":"2022-02-03T12:33:21.068392Z"},"id":"Ot22ujrlLhOd","trusted":true},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir {log_dir}"]},{"cell_type":"markdown","metadata":{"id":"Pe0-8Bzg22ox"},"source":["Finally, run the training loop:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T12:33:24.721013Z","iopub.status.busy":"2022-02-03T12:33:24.720421Z","iopub.status.idle":"2022-02-03T13:56:37.781516Z","shell.execute_reply":"2022-02-03T13:56:37.780704Z","shell.execute_reply.started":"2022-02-03T12:33:24.720966Z"},"id":"a1zZmKmvOH85","trusted":true},"outputs":[],"source":["fit(train_dataset, test_dataset, steps=40000)"]},{"cell_type":"markdown","metadata":{"id":"kz80bY3aQ1VZ"},"source":["## Restore the latest checkpoint and test the network"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T13:56:37.783616Z","iopub.status.busy":"2022-02-03T13:56:37.783350Z","iopub.status.idle":"2022-02-03T13:56:38.618206Z","shell.execute_reply":"2022-02-03T13:56:38.617303Z","shell.execute_reply.started":"2022-02-03T13:56:37.783580Z"},"id":"HSSm4kfvJiqv","trusted":true},"outputs":[],"source":["!ls {checkpoint_dir}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T13:56:38.622546Z","iopub.status.busy":"2022-02-03T13:56:38.621701Z","iopub.status.idle":"2022-02-03T13:56:39.278437Z","shell.execute_reply":"2022-02-03T13:56:39.277639Z","shell.execute_reply.started":"2022-02-03T13:56:38.622499Z"},"id":"4t4x69adQ5xb","trusted":true},"outputs":[],"source":["# Restoring the latest checkpoint in checkpoint_dir\n","checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"]},{"cell_type":"markdown","metadata":{"id":"1RGysMU_BZhx"},"source":["## Generate some images using the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-03T13:56:39.280082Z","iopub.status.busy":"2022-02-03T13:56:39.279829Z","iopub.status.idle":"2022-02-03T13:58:01.426823Z","shell.execute_reply":"2022-02-03T13:58:01.425891Z","shell.execute_reply.started":"2022-02-03T13:56:39.280048Z"},"id":"KUgSnmy2nqSP","trusted":true},"outputs":[],"source":["# Run the trained model on all examples from the test set\n","for inp, tar in test_dataset.take(200):\n","    generate_images(generator, inp, tar)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":4}
